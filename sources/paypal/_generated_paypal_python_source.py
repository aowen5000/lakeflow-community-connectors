# ==============================================================================
# Merged Lakeflow Source: paypal
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import Any, Iterator

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(f"Field {field.name} is not nullable but not found in the input")

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/paypal/paypal.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the PayPal connector with connection-level options.

            Expected options:
                - client_id: OAuth 2.0 client ID from PayPal Developer Dashboard
                - client_secret: OAuth 2.0 client secret from PayPal Developer Dashboard
                - environment (optional): 'sandbox' or 'production'. Defaults to 'sandbox'.
            """
            self.client_id = options.get("client_id")
            self.client_secret = options.get("client_secret")

            if not self.client_id or not self.client_secret:
                raise ValueError(
                    "PayPal connector requires 'client_id' and 'client_secret' in options"
                )

            # Determine base URL based on environment
            environment = options.get("environment", "sandbox").lower()
            if environment == "production":
                self.base_url = "https://api-m.paypal.com"
            else:
                self.base_url = "https://api-m.sandbox.paypal.com"

            # Configure session for API requests
            self._session = requests.Session()
            self._session.headers.update({"Content-Type": "application/json"})

            # Token caching
            self._access_token = None
            self._token_expires_at = None

        def _get_access_token(self) -> str:
            """
            Obtain or refresh OAuth 2.0 access token using client credentials flow.

            Tokens are cached and refreshed 5 minutes before expiration (9-hour lifetime).
            """
            # Check if cached token is still valid (with 5-minute buffer)
            if self._access_token and self._token_expires_at:
                buffer = timedelta(minutes=5)
                if datetime.now() + buffer < self._token_expires_at:
                    return self._access_token

            # Request new token
            token_url = f"{self.base_url}/v1/oauth2/token"

            # Create Basic auth header with Base64-encoded client_id:client_secret
            credentials = f"{self.client_id}:{self.client_secret}"
            encoded_credentials = base64.b64encode(credentials.encode()).decode()

            headers = {
                "Authorization": f"Basic {encoded_credentials}",
                "Content-Type": "application/x-www-form-urlencoded",
            }

            data = {"grant_type": "client_credentials"}

            response = requests.post(token_url, headers=headers, data=data, timeout=30)

            if response.status_code != 200:
                raise RuntimeError(
                    f"PayPal OAuth token request failed: {response.status_code} {response.text}"
                )

            token_data = response.json()
            self._access_token = token_data.get("access_token")
            expires_in = token_data.get("expires_in", 32400)  # Default 9 hours

            self._token_expires_at = datetime.now() + timedelta(seconds=expires_in)

            return self._access_token

        def _make_request(
            self, method: str, endpoint: str, params: dict = None
        ) -> requests.Response:
            """
            Make an authenticated API request to PayPal.

            Args:
                method: HTTP method (GET, POST, etc.)
                endpoint: API endpoint path (e.g., '/v1/reporting/transactions')
                params: Query parameters

            Returns:
                Response object
            """
            access_token = self._get_access_token()

            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json",
            }

            url = f"{self.base_url}{endpoint}"

            response = self._session.request(
                method=method,
                url=url,
                headers=headers,
                params=params,
                timeout=30
            )

            # Handle common error cases
            if response.status_code == 401:
                # Token may have expired, clear cache and retry once
                self._access_token = None
                self._token_expires_at = None
                access_token = self._get_access_token()
                headers["Authorization"] = f"Bearer {access_token}"
                response = self._session.request(
                    method=method,
                    url=url,
                    headers=headers,
                    params=params,
                    timeout=30
                )

            if response.status_code == 429:
                retry_after = response.headers.get("Retry-After", "60")
                raise RuntimeError(
                    f"PayPal API rate limit exceeded. Retry after {retry_after} seconds."
                )

            if response.status_code not in [200, 201]:
                raise RuntimeError(
                    f"PayPal API error: {response.status_code} {response.text}"
                )

            return response

        def list_tables(self) -> list[str]:
            """
            List names of all tables supported by this connector.

            Currently supports only the 'transactions' table.
            """
            return ["transactions"]

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: Additional options (not required for PayPal connector).

            Returns:
                A StructType object representing the schema of the table.
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported table: {table_name!r}")

            if table_name == "transactions":
                # Define nested struct types
                amount_struct = StructType([
                    StructField("currency_code", StringType(), True),
                    StructField("value", StringType(), True),
                ])

                payer_name_struct = StructType([
                    StructField("given_name", StringType(), True),
                    StructField("surname", StringType(), True),
                ])

                payer_info_struct = StructType([
                    StructField("account_id", StringType(), True),
                    StructField("email_address", StringType(), True),
                    StructField("address_status", StringType(), True),
                    StructField("payer_status", StringType(), True),
                    StructField("payer_name", payer_name_struct, True),
                    StructField("country_code", StringType(), True),
                ])

                address_struct = StructType([
                    StructField("line1", StringType(), True),
                    StructField("city", StringType(), True),
                    StructField("country_code", StringType(), True),
                    StructField("postal_code", StringType(), True),
                ])

                shipping_info_struct = StructType([
                    StructField("name", StringType(), True),
                    StructField("address", address_struct, True),
                ])

                transaction_info_struct = StructType([
                    StructField("transaction_id", StringType(), False),
                    StructField("paypal_account_id", StringType(), True),
                    StructField("transaction_event_code", StringType(), True),
                    StructField("transaction_initiation_date", StringType(), True),
                    StructField("transaction_updated_date", StringType(), True),
                    StructField("transaction_amount", amount_struct, True),
                    StructField("fee_amount", amount_struct, True),
                    StructField("transaction_status", StringType(), True),
                    StructField("transaction_subject", StringType(), True),
                    StructField("ending_balance", amount_struct, True),
                    StructField("available_balance", amount_struct, True),
                    StructField("invoice_id", StringType(), True),
                    StructField("custom_field", StringType(), True),
                    StructField("protection_eligibility", StringType(), True),
                ])

                # Item details for cart info
                item_details_struct = StructType([
                    StructField("item_code", StringType(), True),
                    StructField("item_name", StringType(), True),
                    StructField("item_description", StringType(), True),
                    StructField("item_quantity", StringType(), True),
                    StructField("item_unit_price", amount_struct, True),
                    StructField("item_amount", amount_struct, True),
                ])

                cart_info_struct = StructType([
                    StructField("item_details", ArrayType(item_details_struct, True), True),
                ])

                # Main transactions table schema
                transactions_schema = StructType([
                    StructField("transaction_info", transaction_info_struct, True),
                    StructField("payer_info", payer_info_struct, True),
                    StructField("shipping_info", shipping_info_struct, True),
                    StructField("cart_info", cart_info_struct, True),
                ])

                return transactions_schema

            raise ValueError(f"Unsupported table: {table_name!r}")

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch metadata for the given table.

            Args:
                table_name: The name of the table to fetch metadata for.
                table_options: Additional options (not required for PayPal connector).

            Returns:
                A dictionary containing primary_keys, cursor_field, and ingestion_type.
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported table: {table_name!r}")

            if table_name == "transactions":
                return {
                    "primary_keys": ["transaction_info.transaction_id"],
                    "cursor_field": "transaction_info.transaction_initiation_date",
                    "ingestion_type": "snapshot",
                }

            raise ValueError(f"Unsupported table: {table_name!r}")

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read records from a table and return raw JSON-like dictionaries.

            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from.
                table_options: Additional options including start_date and end_date.

            Returns:
                An iterator of records in JSON format and an offset.
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported table: {table_name!r}")

            if table_name == "transactions":
                return self._read_transactions(start_offset, table_options)

            raise ValueError(f"Unsupported table: {table_name!r}")

        def _read_transactions(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Internal implementation for reading the 'transactions' table.

            Required table_options:
                - start_date: ISO 8601 date string (e.g., '2024-01-01T00:00:00Z')
                - end_date: ISO 8601 date string (e.g., '2024-01-31T23:59:59Z')

            Optional table_options:
                - page_size: Number of transactions per page (default: 100, max: 500)

            The PayPal API enforces a maximum 31-day date range per request.
            """
            start_date = table_options.get("start_date")
            end_date = table_options.get("end_date")

            if not start_date or not end_date:
                raise ValueError(
                    "table_options for 'transactions' must include 'start_date' and 'end_date' "
                    "in ISO 8601 format (e.g., '2024-01-01T00:00:00Z')"
                )

            # Validate date range (PayPal enforces 31-day maximum)
            try:
                start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
                end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
                date_range = (end_dt - start_dt).days

                if date_range > 31:
                    raise ValueError(
                        f"Date range exceeds PayPal's 31-day maximum. "
                        f"Requested range: {date_range} days. "
                        f"Please split the date range into smaller windows."
                    )
            except ValueError as e:
                if "31-day" in str(e):
                    raise
                raise ValueError(
                    f"Invalid date format. Expected ISO 8601 format "
                    f"(e.g., '2024-01-01T00:00:00Z'): {e}"
                )

            # Get page size from options (default 100, max 500)
            try:
                page_size = int(table_options.get("page_size", 100))
            except (TypeError, ValueError):
                page_size = 100
            page_size = max(1, min(page_size, 500))

            # Get starting page from offset (default 1)
            if start_offset and isinstance(start_offset, dict):
                page = start_offset.get("page", 1)
            else:
                page = 1

            # Build query parameters
            params = {
                "start_date": start_date,
                "end_date": end_date,
                "page_size": page_size,
                "page": page,
            }

            # Make API request
            response = self._make_request("GET", "/v1/reporting/transactions", params)

            if response.status_code != 200:
                raise RuntimeError(
                    f"PayPal API error for transactions: {response.status_code} {response.text}"
                )

            data = response.json()

            # Extract transaction details array
            transaction_details = data.get("transaction_details", [])
            if not isinstance(transaction_details, list):
                raise ValueError(
                    f"Unexpected response format for transaction_details: "
                    f"{type(transaction_details).__name__}"
                )

            # Process records - keep nested structure, set missing nested objects to None
            records: list[dict[str, Any]] = []
            for txn in transaction_details:
                # Preserve nested structure as returned by API
                record: dict[str, Any] = {
                    "transaction_info": txn.get("transaction_info"),
                    "payer_info": txn.get("payer_info"),
                    "shipping_info": txn.get("shipping_info"),
                    "cart_info": txn.get("cart_info"),
                }
                records.append(record)

            # Determine next offset based on pagination metadata
            total_pages = data.get("total_pages", 1)
            current_page = data.get("page", page)

            # If there are more pages, increment page number
            if current_page < total_pages:
                next_offset = {"page": current_page + 1}
            else:
                # No more pages - return same offset to indicate end of data
                next_offset = start_offset if start_offset else {"page": page}

            return iter(records), next_offset


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
